hydra:
  output_subdir: null  # disable creating .hydra directory
  run:
    dir: .  # disable output directory created by hydra
  job:
    chdir: false  # disable changing working directory

usr_config: null  # e.g. project_root/configs/user_configs/user_config1.yaml

defaults:
  - _self_  # import default.yaml itself
  - datasets: ???  # import dataset
  - override hydra/hydra_logging: disabled   # disable hydra logging because we will use wandb as our logger
  - override hydra/job_logging: disabled   # disable job logging because we will use wandb as our logger

################### Don't modify parameters above #######################

################### You can modify all parameters below #################

wandb:
  enable: true
  api_key: ???  # your wandb api key
  entity: ???  # the place to save your runs. can be your wandb username or team name
  project: ???  # the name of your project
  name: ???  # the name your run

train:  # only valid when running the training script
  epochs: 200
  lr: 0.1
  lr_scheduler:
    enable: true
    which: cosLR  # expLR, stepLR or cosLR
    expLR:
      gamma: 0.95
    stepLR:
      gamma: 0.2  # lr = gamma * lr, when decay step is hit
      decay_step: 50
    cosLR:
      T_max: ${train.epochs}  # maximum epochs
      eta_min: 1e-3  # minimum lr
  optimizer: sgd  # adamw or sgd
  validation_freq: 1  # frequency in epoch(s) to validate the model
  label_smoothing: true
  epsilon: 0.2  # epsilon for label smoothing
  dataloader:
    combine_trainval: true  # combine train and validation set as train set
    batch_size_per_gpu: 4  # the actual batch size should be batch_size_per_gpu * num_gpu
    num_workers: ${train.ddp.nproc_this_node}  # the number of subprocess to load data
    prefetch: ${train.dataloader.batch_size_per_gpu}  # samples to be prefetched. e.g. 64 means 64*num_workers samples to be prefetched
    pin_memory: true  # pin memory in RAM
  ddp:
    which_gpu: [0]
    syn_bn: true  # synchronize batch normalization among gpus
    master_addr: localhost  # don't change this if you use only one PC
    master_port: 12345  # please choose an available port
    nnodes: 1  # how many PCs you want to use
    nproc_this_node: 1  # how many gpu you want to use in current PC, should match 'which_gpu'
    rank_starts_from: 0  # don't change this if you use only one PC
    world_size: 1  # this is equal to 'nproc_this_node' if you only use one PC
  amp: true  # whether to use automatic mixed precision

test:  # only valid when running the test script
  label_smoothing: true
  epsilon: 0.2  # epsilon for label smoothing
  dataloader:
    batch_size_per_gpu: 4
    num_workers: ${test.ddp.nproc_this_node}  # the number of subprocess to load data
    prefetch: ${test.dataloader.batch_size_per_gpu}  # samples to be prefetched. e.g. 64 means 64*num_workers samples to be prefetched
    pin_memory: true  # pin memory in RAM
  ddp:
    which_gpu: [0]
    master_addr: localhost  # don't change this if you use only one PC
    master_port: 12345  # please choose an available port
    nnodes: 1  # how many PCs you want to use
    nproc_this_node: 1  # how many gpu you want to use in current PC, should match 'which_gpu'
    rank_starts_from: 0  # don't change this if you use only one PC
    world_size: 1  # this is equal to 'nproc_this_node' if you only use one PC
  print_results: true
  visualize_preds:
    enable: false
    num_vis: 5  # how many point clouds to visualize for one category
  visualize_neighbors:
    enable: false

edgeconv_block:
  K: [40, 40, 40]
  neighbor_selection_method: [feature, feature, feature]  # coordinate or feature
  group_type: [center_diff, center_diff, center_diff]  # neighbor, diff, center_neighbor or center_diff
  conv1_channel_in: [6, 128, 128]
  conv1_channel_out: [64, 64, 64]
  conv2_channel_in: [64, 64, 64]
  conv2_channel_out: [64, 64, 64]
  pooling: [max, max, max]

embedding:
  which_embedding: edgeconv  # p2p, n2p, linear or edgeconv
  linear:
    emb_in: 3
    emb_out: 64
  neighbor2point_embedding:
    K: 40
    neighbor_selection_method: coordinate  # coordinate or feature
    group_type: diff  # diff, neighbor, center_neighbor or center_diff
    q_in: 3
    q_out: 32
    k_in: 3
    k_out: 32
    v_in: 3
    v_out: 64
    num_heads: 8
  point2point_embedding:
    q_in: 3
    q_out: 32
    k_in: 3
    k_out: 32
    v_in: 3
    v_out: 64
    num_heads: 8
  edgeconv_embedding:
    K: 40
    neighbor_selection_method: coordinate  # coordinate or feature
    group_type: center_diff  # neighbor, diff, center_neighbor or center_diff
    conv1_in: 6
    conv1_out: 64
    conv2_in: 64
    conv2_out: 64
    pooling: max

neighbor2point_block:
  enable: false  # if false, edgeconv block will be used
  neighbor2point:
    scale: [0, 0, 0]
    shared_ca: [false, false, false]
    concat_ms_inputs: [false, false, false]
    mlp_or_ca: [mlp, mlp, mlp]
    K: [40, 40, 40]  # 3 values in the list means neighbor2point_block includes 3 neighbor2point layers. The 'K' for each layer is 40, 40 and 40 respectively
    neighbor_selection_method: [feature, feature, feature]  # coordinate or feature
    group_type: [diff, diff, diff]  # diff, neighbor, center_neighbor or center_diff
    q_in: [64, 64, 64]
    q_out: [64, 64, 64]
    k_in: [64, 64, 64]
    k_out: [64, 64, 64]
    v_in: [64, 64, 64]
    v_out: [64, 64, 64]
    num_heads: [4, 4, 4]
    ff_conv1_channels_in: [64, 64, 64]
    ff_conv1_channels_out: [128, 128, 128]
    ff_conv2_channels_in: [128, 128, 128]
    ff_conv2_channels_out: [64, 64, 64]

conv_block:
  channels_in: [192]
  channels_out: [1024]

point2point_block:
  enable: false  # if false, conv block will be used
  use_embedding: true
  embedding_channels_in: 192
  embedding_channels_out: 1024
  point2point:
    qkv_channels: [1024]
    ff_conv1_channels_in: [1024]
    ff_conv1_channels_out: [512]
    ff_conv2_channels_in: [512]
    ff_conv2_channels_out: [1024]

patch2point:
  enable: true